{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix\n",
    "import collections\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import data_input\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('summaries_dir', 'Summaries', 'Summaries directory')\n",
    "flags.DEFINE_float('learning_rate', 1, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epoch_num', 5, 'Number of epoch.')\n",
    "flags.DEFINE_bool('gpu', 0, \"Enable GPU or not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_normalization(x, phase_train, out_size):\n",
    "    \"\"\"\n",
    "    Batch normalization on convolutional maps.\n",
    "    Ref.: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\n",
    "    Args:\n",
    "        x:           Tensor, 4D BHWD input maps\n",
    "        out_size:       integer, depth of input maps\n",
    "        phase_train: boolean tf.Varialbe, true indicates training phase\n",
    "        scope:       string, variable scope\n",
    "    Return:\n",
    "        normed:      batch-normalized maps\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('bn'):\n",
    "        beta = tf.Variable(tf.constant(0.0, shape=[out_size]),\n",
    "                           name='beta', trainable=True)\n",
    "        gamma = tf.Variable(tf.constant(1.0, shape=[out_size]),\n",
    "                            name='gamma', trainable=True)\n",
    "        batch_mean, batch_var = tf.nn.moments(x, [0], name='moments')\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "        def mean_var_with_update():\n",
    "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "            with tf.control_dependencies([ema_apply_op]):\n",
    "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "\n",
    "        mean, var = tf.cond(phase_train,\n",
    "                            mean_var_with_update,\n",
    "                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
    "        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n",
    "    return normed\n",
    "\n",
    "\n",
    "def variable_summaries(var, name):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "        tf.summary.scalar('sttdev/' + name, stddev)\n",
    "        tf.summary.scalar('max/' + name, tf.reduce_max(var))\n",
    "        tf.summary.scalar('min/' + name, tf.reduce_min(var))\n",
    "        tf.summary.histogram(name, var)\n",
    "        \n",
    "def get_text_summaries():\n",
    "    with tf.name_scope('predict_text'):\n",
    "        predict_strings = tf.placeholder(tf.string,name='predict')\n",
    "        text_summary = tf.summary.text(name='pair',tensor=predict_strings)\n",
    "    return predict_strings,text_summary\n",
    "    \n",
    "\n",
    "def get_evaluate_test_summary():\n",
    "    with tf.name_scope('evaluate'):\n",
    "        evaluate_on_test_acc = tf.placeholder(tf.float32,name='evaluateOnTest')\n",
    "        return evaluate_on_test_acc,tf.summary.scalar('evaluate on test',evaluate_on_test_acc)\n",
    "    \n",
    "\n",
    "def get_loss_summary(name):\n",
    "    with tf.name_scope(name):\n",
    "        average_loss = tf.placeholder(tf.float32)\n",
    "        loss_summary = tf.summary.scalar(name + 'average_loss', average_loss)\n",
    "    return average_loss,loss_summary\n",
    "\n",
    "def input_layer():\n",
    "    \"\"\"\n",
    "    global var:TRIGRAM_D\n",
    "    \"\"\"\n",
    "    with tf.name_scope('input'):\n",
    "        query_in = tf.sparse_placeholder(tf.float32, shape=[None, TRIGRAM_D], name='QueryBatch')\n",
    "        doc_positive_in = tf.sparse_placeholder(tf.float32, shape=[None, TRIGRAM_D], name='DocBatch')\n",
    "        doc_negative_in = tf.sparse_placeholder(tf.float32, shape=[None, TRIGRAM_D], name='DocBatch')\n",
    "        on_train = tf.placeholder(tf.bool)\n",
    "    return query_in,doc_positive_in,doc_negative_in,on_train\n",
    "\n",
    "def batch_layer(query,doc_pos,doc_neg,next_layer_len,on_train,name):\n",
    "    with tf.name_scope(name):\n",
    "        query_layer = batch_normalization(query, on_train, next_layer_len)\n",
    "        doc_positive_layer = batch_normalization(doc_pos, on_train, next_layer_len)\n",
    "        doc_negative_layer = batch_normalization(doc_neg, on_train, next_layer_len)\n",
    "\n",
    "        query_layer_out = tf.nn.relu(query_layer)\n",
    "        doc_positive_layer_out = tf.nn.relu(doc_positive_layer)\n",
    "        doc_negative_layer_out = tf.nn.relu(doc_negative_layer)\n",
    "    return query_layer_out,doc_positive_layer_out,doc_negative_layer_out\n",
    "\n",
    "def fc_layer(query,doc_positive,doc_negative,layer_in_len,layer_out_len,name,first_layer,batch_norm):\n",
    "    with tf.name_scope(name):\n",
    "        layer_par_range = np.sqrt(6.0 / (layer_in_len + layer_out_len))\n",
    "        weight = tf.Variable(tf.random_uniform([layer_in_len, layer_out_len], -layer_par_range, layer_par_range))\n",
    "        bias = tf.Variable(tf.random_uniform([layer_out_len], -layer_par_range, layer_par_range))\n",
    "        variable_summaries(weight, name+'_weights')\n",
    "        variable_summaries(bias, name+'_biases')\n",
    "        \n",
    "        if first_layer:\n",
    "            query_out = tf.sparse_tensor_dense_matmul(query, weight) + bias\n",
    "            doc_positive_out = tf.sparse_tensor_dense_matmul(doc_positive, weight) + bias\n",
    "            doc_negative_out = tf.sparse_tensor_dense_matmul(doc_negative, weight) + bias\n",
    "        else:\n",
    "            query_out = tf.matmul(query, weight) + bias\n",
    "            doc_positive_out = tf.matmul(doc_positive, weight) + bias\n",
    "            doc_negative_out = tf.matmul(doc_negative, weight) + bias\n",
    "        \n",
    "        if batch_norm:\n",
    "            query_out,doc_positive_out,doc_negative_out = batch_layer(query_out,doc_positive_out,doc_negative_out,layer_out_len,True,name+'BN')\n",
    "    return query_out,doc_positive_out,doc_negative_out\n",
    "\n",
    "    \n",
    "def train_loss_layer(query_y,doc_positive_y,doc_negative_y):\n",
    "    \"\"\"\n",
    "    describe: give batch query,doc+,doc- \n",
    "    query_y shape:query_BS,l2_len\n",
    "    doc_positive_y shape:query_BS,l2_len\n",
    "    doc_negative_y shape:query_BS,l2_len\n",
    "    return：\n",
    "        cos_sim : [2,query_BS]\n",
    "        loss: float\n",
    "    \"\"\"\n",
    "    with tf.name_scope('train_Cosine_Similarity'):\n",
    "        \n",
    "            doc_y = tf.concat([doc_positive_y, doc_negative_y], axis=0)\n",
    "\n",
    "            query_norm = tf.tile(tf.sqrt(tf.reduce_sum(tf.square(query_y), 1, True)), [2, 1])\n",
    "            doc_norm = tf.sqrt(tf.reduce_sum(tf.square(doc_y), 1, True))\n",
    "\n",
    "            prod = tf.reduce_sum(tf.multiply(tf.tile(query_y, [2, 1]), doc_y), 1, True)\n",
    "            norm_prod = tf.multiply(query_norm, doc_norm)\n",
    "\n",
    "            # cos_sim_raw = query * doc / (||query|| * ||doc||)\n",
    "            cos_sim_raw = tf.truediv(prod, norm_prod)\n",
    "            cos_sim = tf.transpose(tf.reshape(tf.transpose(cos_sim_raw), [2, query_BS]))  * 20\n",
    "          \n",
    "    with tf.name_scope('train_Loss'):\n",
    "        # Train Loss\n",
    "        # 转化为softmax概率矩阵。\n",
    "        prob = tf.nn.softmax(cos_sim)\n",
    "        # 只取第一列，即正样本列概率。\n",
    "        hit_prob = tf.slice(prob, [0, 0], [-1, 1])\n",
    "        loss = -tf.reduce_sum(tf.log(hit_prob))\n",
    "        tf.summary.scalar('loss', loss)\n",
    "    return cos_sim,prob,loss\n",
    "\n",
    "def triple_loss_layer(query_y,doc_positive_y,doc_negative_y):\n",
    "    \"\"\"\n",
    "    describe: give batch query,doc+,doc- \n",
    "    query_y shape:1,l2_len\n",
    "    doc_positive_y shape:1,l2_len\n",
    "    doc_negative_y shape:1,l2_len\n",
    "    return：\n",
    "        cos_sim : [2,1]\n",
    "        loss: float\n",
    "    \"\"\"\n",
    "    doc_y = tf.concat([doc_positive_y, doc_negative_y], axis=0)\n",
    "    query_norm = tf.tile(tf.sqrt(tf.reduce_sum(tf.square(query_y), 1, True)), [2, 1])\n",
    "    doc_norm = tf.sqrt(tf.reduce_sum(tf.square(doc_y), 1, True))\n",
    "    \n",
    "    prod = tf.reduce_sum(tf.multiply(tf.tile(query_y, [2, 1]), doc_y), 1, True)\n",
    "    norm_prod = tf.multiply(query_norm, doc_norm)\n",
    "\n",
    "    # cos_sim_raw = query * doc / (||query|| * ||doc||)\n",
    "    cos_sim_raw = tf.truediv(prod, norm_prod)\n",
    "    \n",
    "    cos_sim = tf.transpose(tf.reshape(tf.transpose(cos_sim_raw), [2, 1])) * 20\n",
    "    \n",
    "    prob = tf.nn.softmax(cos_sim)\n",
    "    # 只取第一列，即正样本列概率。\n",
    "    hit_prob = tf.slice(prob, [0, 0], [-1, 1])\n",
    "    loss = -tf.reduce_sum(tf.log(hit_prob))\n",
    "    return cos_sim,loss\n",
    "\n",
    "def accuracy_layer(prob):\n",
    "    correct_prediction = tf.equal(tf.argmax(prob, 1), 0)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    return accuracy\n",
    "\n",
    "def predict_layer(query_y,doc_positive_y):\n",
    "    \"\"\"\n",
    "    describe: give batch query,doc+\n",
    "    query_y shape:1,l2_len\n",
    "    doc_positive_y shape:main_question_len,l2_len\n",
    "    return：\n",
    "        cos_sim : [main_len,1]\n",
    "        loss: float\n",
    "    \"\"\"\n",
    "    # Cosine similarity\n",
    "    query_norm = tf.tile(tf.sqrt(tf.reduce_sum(tf.square(query_y), 1, True)), [MAIN_QUESTION_NUM, 1])\n",
    "    doc_norm = tf.sqrt(tf.reduce_sum(tf.square(doc_positive_y), 1, True))\n",
    "\n",
    "    prod = tf.reduce_sum(tf.multiply(tf.tile(query_y, [MAIN_QUESTION_NUM, 1]), doc_positive_y), 1, True)\n",
    "    norm_prod = tf.multiply(query_norm, doc_norm)\n",
    "\n",
    "    # cos_sim_raw = query * doc / (||query|| * ||doc||)\n",
    "    cos_sim_raw = tf.truediv(prod, norm_prod)\n",
    "\n",
    "    cos_sim = tf.transpose(tf.reshape(tf.transpose(cos_sim_raw), [MAIN_QUESTION_NUM, 1])) \n",
    "    \n",
    "    prob = tf.nn.softmax(cos_sim)\n",
    "    \n",
    "    label = tf.argmax(prob,1)[0]\n",
    "    return prob,label\n",
    "\n",
    "\n",
    "\n",
    "def pull_all(index_list):\n",
    "    #该地方插入函数，把query_iin，doc_positive_in,doc_negative_in转化成one_hot，再转化成coo_matrix\n",
    "    query_in = data_set.get_one_hot_from_batch(index_list,'query')\n",
    "    doc_positive_in = data_set.get_one_hot_from_batch(index_list,'main_question')\n",
    "    doc_negative_in = data_set.get_one_hot_from_batch(index_list,'other_question')\n",
    "    \n",
    "    query_in = coo_matrix(query_in)\n",
    "    doc_positive_in = coo_matrix(doc_positive_in)\n",
    "    doc_negative_in = coo_matrix(doc_negative_in)\n",
    "    \n",
    "    query_in = tf.SparseTensorValue(\n",
    "        np.transpose([np.array(query_in.row, dtype=np.int64), np.array(query_in.col, dtype=np.int64)]),\n",
    "        np.array(query_in.data, dtype=np.float),\n",
    "        np.array(query_in.shape, dtype=np.int64))\n",
    "    doc_positive_in = tf.SparseTensorValue(\n",
    "        np.transpose([np.array(doc_positive_in.row, dtype=np.int64), np.array(doc_positive_in.col, dtype=np.int64)]),\n",
    "        np.array(doc_positive_in.data, dtype=np.float),\n",
    "        np.array(doc_positive_in.shape, dtype=np.int64))\n",
    "    doc_negative_in = tf.SparseTensorValue(\n",
    "        np.transpose([np.array(doc_negative_in.row, dtype=np.int64), np.array(doc_negative_in.col, dtype=np.int64)]),\n",
    "        np.array(doc_negative_in.data, dtype=np.float),\n",
    "        np.array(doc_negative_in.shape, dtype=np.int64))\n",
    "\n",
    "    return query_in, doc_positive_in, doc_negative_in\n",
    "\n",
    "\n",
    "def pull_batch(index_list,batch_id):\n",
    "    \n",
    "    if (batch_id + 1) * query_BS >= len(index_list):\n",
    "        print \"batch outof index\"\n",
    "        return None\n",
    "    \n",
    "    batch_index_list = index_list[batch_id * query_BS:(batch_id + 1) * query_BS]\n",
    "    query_in, doc_positive_in, doc_negative_in = pull_all(batch_index_list)\n",
    "    return query_in, doc_positive_in, doc_negative_in\n",
    "\n",
    "\n",
    "def feed_dict(train_index_list,test_index_list,on_training, Train, batch_id):\n",
    "    \"\"\"\n",
    "    input: data_sets is a dict and the value type is numpy\n",
    "    describe: to match the text classification the data_sets's content is the doc in df\n",
    "    \"\"\"\n",
    "    if Train:\n",
    "        query, doc_positive, doc_negative = pull_batch(train_index_list,batch_id)\n",
    "        \n",
    "    else:\n",
    "        query, doc_positive, doc_negative = pull_batch(test_index_list,batch_id)\n",
    "        \n",
    "    return {query_in: query, doc_positive_in: doc_positive, doc_negative_in: doc_negative,\n",
    "            on_train: on_training}\n",
    "\n",
    "def feed_evaluate_dict(sentence,on_training=True):\n",
    "    \"\"\"\n",
    "    input: data_sets is a dict and the value type is numpy\n",
    "    describe: to match the text classification the data_sets's content is the doc in df\n",
    "    \"\"\"\n",
    "    #该地方插入函数，把query_iin，doc_positive_in,doc_negative_in转化成one_hot，再转化成coo_matrix\n",
    "    query = data_set.get_one_hot_from_sentence(sentence)\n",
    "    doc_positive = data_set.get_one_hot_from_main_question()\n",
    "#     doc_negative = np.ones((1,data_set.get_word_num()))\n",
    "    \n",
    "    query = coo_matrix(query)\n",
    "    doc_positive = coo_matrix(doc_positive)\n",
    "#     doc_negative = coo_matrix(doc_negative)\n",
    "    \n",
    "    query = tf.SparseTensorValue(\n",
    "        np.transpose([np.array(query.row, dtype=np.int64), np.array(query.col, dtype=np.int64)]),\n",
    "        np.array(query.data, dtype=np.float),\n",
    "        np.array(query.shape, dtype=np.int64))\n",
    "    doc_positive = tf.SparseTensorValue(\n",
    "        np.transpose([np.array(doc_positive.row, dtype=np.int64), np.array(doc_positive.col, dtype=np.int64)]),\n",
    "        np.array(doc_positive.data, dtype=np.float),\n",
    "        np.array(doc_positive.shape, dtype=np.int64))\n",
    "#     doc_negative = tf.SparseTensorValue(\n",
    "#         np.transpose([np.array(doc_negative.row, dtype=np.int64), np.array(doc_negative.col, dtype=np.int64)]),\n",
    "#         np.array(doc_negative.data, dtype=np.float),\n",
    "#         np.array(doc_negative.shape, dtype=np.int64))\n",
    "    \n",
    "    return {query_in: query, doc_positive_in: doc_positive,on_train: on_training}\n",
    "\n",
    "def feed_triple_dict(query,doc_pos,doc_neg,on_training=True):\n",
    "    \"\"\"\n",
    "    input: data_sets is a dict and the value type is numpy\n",
    "    describe: to match the text classification the data_sets's content is the doc in df\n",
    "    \"\"\"\n",
    "    #该地方插入函数，把query_iin，doc_positive_in,doc_negative_in转化成one_hot，再转化成coo_matrix\n",
    "    query = data_set.get_one_hot_from_sentence(query)\n",
    "    doc_positive = data_set.get_one_hot_from_sentence(doc_pos)\n",
    "    doc_negative = data_set.get_one_hot_from_sentence(doc_neg)\n",
    "    \n",
    "    query = coo_matrix(query)\n",
    "    doc_positive = coo_matrix(doc_positive)\n",
    "    doc_negative = coo_matrix(doc_negative)\n",
    "    \n",
    "    query = tf.SparseTensorValue(\n",
    "        np.transpose([np.array(query.row, dtype=np.int64), np.array(query.col, dtype=np.int64)]),\n",
    "        np.array(query.data, dtype=np.float),\n",
    "        np.array(query.shape, dtype=np.int64))\n",
    "    doc_positive = tf.SparseTensorValue(\n",
    "        np.transpose([np.array(doc_positive.row, dtype=np.int64), np.array(doc_positive.col, dtype=np.int64)]),\n",
    "        np.array(doc_positive.data, dtype=np.float),\n",
    "        np.array(doc_positive.shape, dtype=np.int64))\n",
    "    doc_negative = tf.SparseTensorValue(\n",
    "        np.transpose([np.array(doc_negative.row, dtype=np.int64), np.array(doc_negative.col, dtype=np.int64)]),\n",
    "        np.array(doc_negative.data, dtype=np.float),\n",
    "        np.array(doc_negative.shape, dtype=np.int64))\n",
    "    \n",
    "    return {query_in: query, doc_positive_in: doc_positive, doc_negative_in: doc_negative,on_train: on_training}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "    global var : epoch_num train_index_list test_index_list train_size test_size query_BS\n",
    "    \"\"\"\n",
    "    config = tf.ConfigProto() \n",
    "    config.gpu_options.allow_growth = True\n",
    "    if not FLAGS.gpu:\n",
    "        print \"we use gpu\"\n",
    "        config = tf.ConfigProto(device_count= {'GPU' : 0},allow_soft_placement=True)\n",
    "\n",
    "    # 创建一个Saver对象，选择性保存变量或者模型。\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session(config=config) as sess:\n",
    "        \n",
    "        print \"variable initial\"\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print \"variable initial ok!\"\n",
    "        \n",
    "        train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/train', sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/test', sess.graph)\n",
    "\n",
    "        print \"start training\"\n",
    "        for epoch_id in range(FLAGS.epoch_num):\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            start = time.time()\n",
    "            next_time = time.time()\n",
    "            for batch_id in range(int(train_size/query_BS)):\n",
    "                summary,_,loss_v,acc_v = sess.run([merged,train_step,loss,accuracy], feed_dict=feed_dict(train_index_list,test_index_list,True, True, batch_id))    \n",
    "                train_writer.add_summary(summary, batch_id + 1)\n",
    "                epoch_loss += loss_v\n",
    "                epoch_acc += acc_v\n",
    "                if batch_id % 500 == 0:\n",
    "                    this_time = time.time()\n",
    "                    print (\"batch_id:%d  loss:%f time:%f\")%(batch_id,loss_v,this_time-next_time)\n",
    "                    start_time_inbatch = time.time()\n",
    "                    #add text_summary\n",
    "                    query_list = random.sample(list(data_set.df['query']),10)\n",
    "                    text_summaries_t = sess.run(text_summary,feed_dict={predict_strings:predict_label_n_with_sess(sess,query_list)})\n",
    "                    train_writer.add_summary(text_summaries_t,int(train_size/query_BS) * epoch_id + batch_id+1)\n",
    "                    #add evaluate_test()\n",
    "                    evaluae_summary_t = sess.run(evaluae_summary,feed_dict={evaluate_on_test_acc:evaluate_test_with_sess(sess,'data/train_data_toy.csv')})\n",
    "                    train_writer.add_summary(evaluae_summary_t,batch_id+1)   \n",
    "                    print (\"this evaluate cost time :%f\"%(time.time() -  start_time_inbatch ))\n",
    "                    next_time = time.time()\n",
    "\n",
    "\n",
    "            end = time.time()\n",
    "            epoch_loss /= int(train_size/query_BS)\n",
    "            epoch_acc /= int(train_size/query_BS)\n",
    "            print(\"\\nEpoch #%-5d | Train Loss: %-4.3f | PureTrainTime: %-3.3fs | acc: %f\" %\n",
    "                  (epoch_id, epoch_loss, end - start,epoch_acc))\n",
    "\n",
    "            # test loss\n",
    "            start = time.time()\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            for batch_id in range(int(test_size/query_BS)):\n",
    "                summary,loss_v,acc_v = sess.run([merged,loss,accuracy], feed_dict=feed_dict(train_index_list,test_index_list,False, False, batch_id))\n",
    "                test_writer.add_summary(summary, batch_id + 1)\n",
    "                epoch_loss += loss_v\n",
    "                epoch_acc += acc_v\n",
    "            end = time.time()\n",
    "            epoch_loss /= int(test_size/query_BS)\n",
    "            epoch_acc /= int(test_size/query_BS)\n",
    "            print(\"Epoch #%-5d | Test  Loss: %-4.3f | Calc_LossTime: %-3.3fs | acc: %f\" %\n",
    "                  (epoch_id, epoch_loss,end - start,epoch_acc))\n",
    "\n",
    "        # 保存模型\n",
    "        save_path = saver.save(sess, \"model/model_1.ckpt\")\n",
    "        print(\"Model saved in file: \", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#这里之后必要时写成类，现在还不能当库用，里面很多默认的全局。\n",
    "\n",
    "#根据句子，预测主问题\n",
    "def predict_label(sentence,view=True):\n",
    "    \"\"\"\n",
    "    class fun flag\n",
    "    global var: pred_prob,pred_label dataset\n",
    "    \"\"\"\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    #写一个函数查看输入query和输出类别\n",
    "    config = tf.ConfigProto() \n",
    "    config.gpu_options.allow_growth = True\n",
    "    if not FLAGS.gpu:\n",
    "        config = tf.ConfigProto(device_count= {'GPU' : 0},allow_soft_placement=True)\n",
    "    with tf.Session(config=config) as sess:     \n",
    "        saver.restore(sess, \"model/model_1.ckpt\")\n",
    "        print \"Model restored.\"\n",
    "        pred_prob_v,pred_label_v = sess.run([pred_prob,pred_label],feed_dict=feed_evaluate_dict(sentence))\n",
    "        pred_main_question = data_set.get_main_question_from_label_index(pred_label_v)\n",
    "        if view:\n",
    "            print sentence,pred_main_question,pred_label_v\n",
    "    return pred_main_question\n",
    "\n",
    "def predict_label_n_with_sess(sess,sentence_list):\n",
    "    result_list = []\n",
    "    for i,sentence in enumerate(sentence_list):\n",
    "        pred_prob_v,pred_label_v = sess.run([pred_prob,pred_label],feed_dict=feed_evaluate_dict(sentence))\n",
    "        pred_main_question = data_set.get_main_question_from_label_index(pred_label_v)\n",
    "        result_list.append(sentence + \":\" +pred_main_question)\n",
    "    return result_list\n",
    "        \n",
    "#测试主问题的正确匹配度\n",
    "def evaluate_main_question():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    #写一个函数查看输入query和输出类别\n",
    "    config = tf.ConfigProto() \n",
    "    config.gpu_options.allow_growth = True\n",
    "    if not FLAGS.gpu:\n",
    "        config = tf.ConfigProto(device_count= {'GPU' : 0},allow_soft_placement=True)\n",
    "    with tf.Session(config=config) as sess:     \n",
    "        saver.restore(sess, \"model/model_1.ckpt\")\n",
    "        print \"Model restored.\"\n",
    "        \n",
    "        count = 0\n",
    "        acc = 0\n",
    "        for i,sentence in enumerate(data_set.get_main_question_list()):\n",
    "            pred_prob_v,pred_label_v = sess.run([pred_prob,pred_label],feed_dict=feed_evaluate_dict(sentence))\n",
    "            pred_main_question = data_set.get_main_question_from_label_index(pred_label_v)\n",
    "            if sentence == pred_main_question:\n",
    "                acc += 1\n",
    "            count += 1\n",
    "        print acc/float(count),count\n",
    "        \n",
    "#查看一个triple的loss\n",
    "def show_triple_loss(query,doc_pos,doc_neg):\n",
    "    \"\"\"\n",
    "    class flag\n",
    "    global var: query_y,doc_pos doc_neg\n",
    "    \"\"\"\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    #写一个函数查看输入query和输出类别\n",
    "    config = tf.ConfigProto() \n",
    "    config.gpu_options.allow_growth = True\n",
    "    if not FLAGS.gpu:\n",
    "        config = tf.ConfigProto(device_count= {'GPU' : 0},allow_soft_placement=True)\n",
    "    with tf.Session(config=config) as sess:     \n",
    "        saver.restore(sess, \"model/model_1.ckpt\")\n",
    "        cos_sim,loss = triple_loss_layer(query_y,doc_positive_y,doc_negative_y)\n",
    "        return  sess.run([cos_sim,loss],feed_dict=feed_triple_dict(query,doc_pos,doc_neg))\n",
    "\n",
    "#对所有log进行测试\n",
    "#写一个测评脚本，测试真实情况与可视化真实情况\n",
    "def evaluate_test(test_data_path,view=True):\n",
    "    print \"start evaluate test func\"\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    #写一个函数查看输入query和输出类别\n",
    "    config = tf.ConfigProto() \n",
    "    config.gpu_options.allow_growth = True\n",
    "    if not FLAGS.gpu:\n",
    "        config = tf.ConfigProto(device_count= {'GPU' : 0},allow_soft_placement=True)\n",
    "        print \"use_gpu\"\n",
    "    with tf.Session(config=config) as sess:     \n",
    "        saver.restore(sess, \"model/model_1.ckpt\")\n",
    "        print \"Model restored.\"\n",
    "        \n",
    "        count = 0\n",
    "        acc = 0\n",
    "        df_test = pd.read_csv(test_data_path,encoding='utf-8')\n",
    "        test_question_query_list = list(df_test['query'])\n",
    "        test_question_label_list = list(df_test['main_question'])\n",
    "        for i,sentence in enumerate(test_question_query_list):\n",
    "            pred_prob_v,pred_label_v = sess.run([pred_prob,pred_label],feed_dict=feed_evaluate_dict(sentence))\n",
    "            pred_main_question = data_set.get_main_question_from_label_index(pred_label_v)\n",
    "            if pred_main_question == test_question_label_list[i]:\n",
    "                acc += 1\n",
    "#             else:\n",
    "#                 print sentence,pred_main_question,test_question_label_list[i]\n",
    "            count += 1\n",
    "#             if i % 1000 == 0:\n",
    "#                 print i\n",
    "        if view:\n",
    "            print acc/float(count),count\n",
    "    return acc/float(count)\n",
    "        \n",
    "def evaluate_test_with_sess(sess,test_data_path):\n",
    "    count = 0\n",
    "    acc = 0\n",
    "    df_test = pd.read_csv(test_data_path,encoding='utf-8')\n",
    "    test_question_query_list = list(df_test['query'])\n",
    "    test_question_label_list = list(df_test['main_question'])\n",
    "    for i,sentence in enumerate(test_question_query_list):\n",
    "        pred_prob_v,pred_label_v = sess.run([pred_prob,pred_label],feed_dict=feed_evaluate_dict(sentence))\n",
    "        pred_main_question = data_set.get_main_question_from_label_index(pred_label_v)\n",
    "        if pred_main_question == test_question_label_list[i]:\n",
    "            acc += 1\n",
    "        count += 1\n",
    "    return acc/float(count)\n",
    "    \n",
    "#查看中间层\n",
    "def show_var_from_sentence(sentence):\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    #写一个函数查看输入query和输出类别\n",
    "    config = tf.ConfigProto() \n",
    "    config.gpu_options.allow_growth = True\n",
    "    if not FLAGS.gpu:\n",
    "        config = tf.ConfigProto(device_count= {'GPU' : 0},allow_soft_placement=True)\n",
    "    with tf.Session(config=config) as sess:     \n",
    "        saver.restore(sess, \"model/model_1.ckpt\")\n",
    "        return  sess.run(query_l1_out,feed_dict=feed_evaluate_dict(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read over,time:0.0194451808929\n",
      "word_dict_num:1175\n",
      "build dict over,time:0.0133109092712\n",
      "split over,time:0.316452980042\n",
      "train_size and test_size:140909 60465\n",
      "generate_triple_over,time:0.0178220272064\n"
     ]
    }
   ],
   "source": [
    "#测试dataset\n",
    "from data_input_fast import Data_set\n",
    "data_set = Data_set(data_path='data/train_data_toy.csv',data_percent=0.5,train_percent=0.7)\n",
    "train_size, test_size = data_set.get_train_test_size()\n",
    "train_index_list = data_set.train_index_list\n",
    "test_index_list = data_set.test_index_list\n",
    "\n",
    "TRIGRAM_D = data_set.get_word_num()\n",
    "MAIN_QUESTION_NUM = data_set.get_main_question_num()\n",
    "\n",
    "query_BS = 100\n",
    "L1_N = 400\n",
    "L2_N = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name evaluate on test is illegal; using evaluate_on_test instead.\n"
     ]
    }
   ],
   "source": [
    "#input\n",
    "query_in,doc_positive_in,doc_negative_in,on_train = input_layer()\n",
    "#fc1 - bn?\n",
    "query_layer1_out,doc_pos_layer1_out,doc_neg_layer1_out = fc_layer(query_in,doc_positive_in,doc_negative_in,TRIGRAM_D,L1_N,'FC1',True,False)\n",
    "#fc2 - bn?\n",
    "query_y,doc_positive_y,doc_negative_y = fc_layer(query_layer1_out,doc_pos_layer1_out,doc_neg_layer1_out,L1_N,L2_N,'FC2',False,False)\n",
    "#loss\n",
    "cos_sim,prob,loss = train_loss_layer(query_y,doc_positive_y,doc_negative_y)\n",
    "#acc\n",
    "accuracy = accuracy_layer(prob)\n",
    "#pred_label\n",
    "pred_prob,pred_label = predict_layer(query_y,doc_positive_y)\n",
    "# Optimizer\n",
    "train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(loss)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "#evaluate\n",
    "evaluate_on_test_acc,evaluae_summary = get_evaluate_test_summary()\n",
    "#record predict text\n",
    "predict_strings,text_summary = get_text_summaries()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we use gpu\n",
      "variable initial\n",
      "variable initial ok!\n",
      "start training\n",
      "batch_id:0  loss:36.896549 time:1522311854.541243\n",
      "batch_id:500  loss:8.982119 time:120.091655\n",
      "batch_id:1000  loss:2.658369 time:124.391078\n",
      "\n",
      "Epoch #0     | Train Loss: 11.722 | PureTrainTime: 356.591s | acc: 0.971107\n",
      "Epoch #0     | Test  Loss: 2.434 | Calc_LossTime: 75.504s | acc: 0.994387\n",
      "batch_id:0  loss:1.502890 time:1522312286.575266\n",
      "batch_id:500  loss:4.811432 time:127.847927\n",
      "batch_id:1000  loss:1.874596 time:126.705200\n",
      "\n",
      "Epoch #1     | Train Loss: 1.789 | PureTrainTime: 370.993s | acc: 0.996089\n",
      "Epoch #1     | Test  Loss: 1.585 | Calc_LossTime: 71.183s | acc: 0.996821\n",
      "batch_id:0  loss:1.178385 time:1522312728.764567\n",
      "batch_id:500  loss:2.443932 time:105.549021\n",
      "batch_id:1000  loss:1.876015 time:107.049535\n",
      "\n",
      "Epoch #2     | Train Loss: 1.320 | PureTrainTime: 308.643s | acc: 0.997644\n",
      "Epoch #2     | Test  Loss: 1.374 | Calc_LossTime: 67.602s | acc: 0.997682\n",
      "batch_id:0  loss:1.376796 time:1522313105.047690\n",
      "batch_id:500  loss:0.722773 time:106.337488\n",
      "batch_id:1000  loss:1.649679 time:106.766897\n",
      "\n",
      "Epoch #3     | Train Loss: 1.188 | PureTrainTime: 309.009s | acc: 0.998105\n",
      "Epoch #3     | Test  Loss: 1.439 | Calc_LossTime: 68.476s | acc: 0.997285\n",
      "batch_id:0  loss:0.876926 time:1522313482.497531\n",
      "batch_id:500  loss:1.556344 time:106.859903\n",
      "batch_id:1000  loss:1.730198 time:107.358379\n",
      "\n",
      "Epoch #4     | Train Loss: 1.035 | PureTrainTime: 310.578s | acc: 0.998708\n",
      "Epoch #4     | Test  Loss: 1.296 | Calc_LossTime: 67.817s | acc: 0.997632\n",
      "('Model saved in file: ', 'model/model_1.ckpt')\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model_1.ckpt\n",
      "Model restored.\n",
      "请问金卡能退吗 会员卡怎么取消 42\n",
      "会员卡怎么取消\n"
     ]
    }
   ],
   "source": [
    "predict_label('请问金卡能退吗')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model_1.ckpt\n",
      "Model restored.\n",
      "1.0 100\n"
     ]
    }
   ],
   "source": [
    "#主问题测试\n",
    "evaluate_main_question()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_gpu\n",
      "INFO:tensorflow:Restoring parameters from model/model_1.ckpt\n",
      "Model restored.\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "0.884663186677 4023\n"
     ]
    }
   ],
   "source": [
    "#测试集测试\n",
    "evaluate_test('data/train_data_toy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model_1.ckpt\n",
      "[array([[ 19.0454464 ,   1.01518691]], dtype=float32), -0.0]\n",
      "INFO:tensorflow:Restoring parameters from model/model_1.ckpt\n",
      "[array([[ 16.85266113,  10.40456963]], dtype=float32), 0.001582324]\n",
      "INFO:tensorflow:Restoring parameters from model/model_1.ckpt\n",
      "[array([[  2.25196958,  20.        ]], dtype=float32), 17.74803]\n"
     ]
    }
   ],
   "source": [
    "#测试triple-loss\n",
    "query = '会员卡'\n",
    "doc_pos = '会员卡规则'\n",
    "doc_neg = '很高兴认识你'\n",
    "print show_triple_loss(query,doc_pos,doc_neg)\n",
    "\n",
    "query = '我要退卡'\n",
    "doc_pos = '会员卡取消'\n",
    "doc_neg = '会员卡规则'\n",
    "print show_triple_loss(query,doc_pos,doc_neg)\n",
    "\n",
    "query = '账号验证'\n",
    "doc_pos = '很高兴认识你'\n",
    "doc_neg = '会员卡规则'\n",
    "print show_triple_loss(query,doc_pos,doc_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model_1.ckpt\n",
      "Model restored.\n",
      "给我寄到新地方 给一下卖家的联系方式 62\n"
     ]
    }
   ],
   "source": [
    "predict_label('给我寄到新地方')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 测当前step，测试集上的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是?\n"
     ]
    }
   ],
   "source": [
    "string = tf.placeholder(tf.string)\n",
    "with tf.Session() as sess:\n",
    "    print  sess.run(string,feed_dict={string:\"我是?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.summary.text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
